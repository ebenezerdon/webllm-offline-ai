# ğŸ¤– Browser LLM Chat

An offline AI assistant that runs entirely in your browser using WebLLM and WebGPU! âœ¨

This application allows you to chat with LLMs directly in your browser without sending data to external servers. All processing happens locally on your device.

## âš™ï¸ Requirements

- ğŸŒ A modern browser with WebGPU support:
  - Chrome 113+
  - Edge 113+
  - Firefox 118+
- ğŸ’» A device with sufficient GPU capabilities
- ğŸ’¾ Approximately 1-4GB of storage space (depending on model size)

## ğŸš€ How to Use

1. Simply open the `index.html` file in a supported browser
2. Select a model from the dropdown menu
3. Click "Load Model" and wait for the download to complete
4. Start chatting with the AI! ğŸ’¬

## ğŸ”§ Technical Details

This app uses:

- ğŸ§  WebLLM library to run models in the browser
- âš¡ WebGPU for hardware acceleration
- ğŸ—œï¸ Quantized models for efficient performance
- ğŸ“ Basic HTML, CSS, and JavaScript (no framework dependencies)

## ğŸ¤– Models

- **SmolLM2 360M**: ğŸ¥ A very small model, great for basic tasks
- **Llama 3.1 8B**: ğŸ¦™ Medium-sized model with good capabilities
- **Phi 3.5 Mini**: ğŸ¦Š Larger model with enhanced response quality

## ğŸ‘¨â€ğŸ’» Development

Feel free to modify the app to suit your needs. The entire application is contained in a single HTML file for simplicity.

## ğŸ“œ License

This project is open source and available under the MIT License.

## âœ¨ Enhanced Version

For a more feature-rich implementation with additional functionality, check out:

- [WebLLM Offline AI Assistant](https://github.com/ebenezerdon/webllm-offline-ai) - A more advanced version with:
  - ğŸ–¥ï¸ PC-themed desktop interface
  - ğŸ’¬ Chat history support
  - ğŸ—ƒï¸ IndexedDB caching
  - ğŸ“ Logger
  - ğŸ–±ï¸ Draggable windows
  - ğŸ”½ Taskbar and window controls
  - ğŸ“± Responsive design for mobile and desktop

âœ¨ Live demo: [chat.ebenezerdon.com](https://chat.ebenezerdon.com)
